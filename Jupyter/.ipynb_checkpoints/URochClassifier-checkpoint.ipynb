{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b80d94da-8471-4b9e-815a-658170df3fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: natsort in c:\\users\\13523\\venvs\\lab\\lib\\site-packages (8.4.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install natsort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "626d2532-5601-41fe-a688-6b3b752d1436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] C:\\Users\\13523\\Desktop\\URochDataset_trimmed\\_manifest_index_only.csv\n",
      "   piece_index    piece folder_instr category  track instrument   ext  \\\n",
      "0           01  Jupiter        vn_vc    AuMix    NaN      vn_vc  .wav   \n",
      "1           01  Jupiter        vn_vc    AuSep    1.0       None  .wav   \n",
      "2           01  Jupiter        vn_vc    AuSep    2.0       None  .wav   \n",
      "3           01  Jupiter        vn_vc      F0s    1.0       None  .txt   \n",
      "4           01  Jupiter        vn_vc      F0s    2.0       None  .txt   \n",
      "5           01  Jupiter        vn_vc    Notes    1.0       None  .txt   \n",
      "6           01  Jupiter        vn_vc    Notes    2.0       None  .txt   \n",
      "7           01  Jupiter        vn_vc    Score    NaN      vn_vc  .mid   \n",
      "8           01  Jupiter        vn_vc    Score    NaN      vn_vc  .pdf   \n",
      "9           01  Jupiter        vn_vc    Video    NaN      vn_vc  .mp4   \n",
      "10          02   Sonata        vn_vn    AuMix    NaN      vn_vn  .wav   \n",
      "11          02   Sonata        vn_vn    AuSep    1.0       None  .wav   \n",
      "12          02   Sonata        vn_vn    AuSep    2.0       None  .wav   \n",
      "13          02   Sonata        vn_vn      F0s    1.0       None  .txt   \n",
      "14          02   Sonata        vn_vn      F0s    2.0       None  .txt   \n",
      "15          02   Sonata        vn_vn    Notes    1.0       None  .txt   \n",
      "16          02   Sonata        vn_vn    Notes    2.0       None  .txt   \n",
      "17          02   Sonata        vn_vn    Score    NaN      vn_vn  .mid   \n",
      "18          02   Sonata        vn_vn    Score    NaN      vn_vn  .pdf   \n",
      "19          02   Sonata        vn_vn    Video    NaN      vn_vn  .mp4   \n",
      "\n",
      "                                                 path            folder  \n",
      "0   C:\\Users\\13523\\Desktop\\URochDataset_trimmed\\01...  01_Jupiter_vn_vc  \n",
      "1   C:\\Users\\13523\\Desktop\\URochDataset_trimmed\\01...  01_Jupiter_vn_vc  \n",
      "2   C:\\Users\\13523\\Desktop\\URochDataset_trimmed\\01...  01_Jupiter_vn_vc  \n",
      "3   C:\\Users\\13523\\Desktop\\URochDataset_trimmed\\01...  01_Jupiter_vn_vc  \n",
      "4   C:\\Users\\13523\\Desktop\\URochDataset_trimmed\\01...  01_Jupiter_vn_vc  \n",
      "5   C:\\Users\\13523\\Desktop\\URochDataset_trimmed\\01...  01_Jupiter_vn_vc  \n",
      "6   C:\\Users\\13523\\Desktop\\URochDataset_trimmed\\01...  01_Jupiter_vn_vc  \n",
      "7   C:\\Users\\13523\\Desktop\\URochDataset_trimmed\\01...  01_Jupiter_vn_vc  \n",
      "8   C:\\Users\\13523\\Desktop\\URochDataset_trimmed\\01...  01_Jupiter_vn_vc  \n",
      "9   C:\\Users\\13523\\Desktop\\URochDataset_trimmed\\01...  01_Jupiter_vn_vc  \n",
      "10  C:\\Users\\13523\\Desktop\\URochDataset_trimmed\\02...   02_Sonata_vn_vn  \n",
      "11  C:\\Users\\13523\\Desktop\\URochDataset_trimmed\\02...   02_Sonata_vn_vn  \n",
      "12  C:\\Users\\13523\\Desktop\\URochDataset_trimmed\\02...   02_Sonata_vn_vn  \n",
      "13  C:\\Users\\13523\\Desktop\\URochDataset_trimmed\\02...   02_Sonata_vn_vn  \n",
      "14  C:\\Users\\13523\\Desktop\\URochDataset_trimmed\\02...   02_Sonata_vn_vn  \n",
      "15  C:\\Users\\13523\\Desktop\\URochDataset_trimmed\\02...   02_Sonata_vn_vn  \n",
      "16  C:\\Users\\13523\\Desktop\\URochDataset_trimmed\\02...   02_Sonata_vn_vn  \n",
      "17  C:\\Users\\13523\\Desktop\\URochDataset_trimmed\\02...   02_Sonata_vn_vn  \n",
      "18  C:\\Users\\13523\\Desktop\\URochDataset_trimmed\\02...   02_Sonata_vn_vn  \n",
      "19  C:\\Users\\13523\\Desktop\\URochDataset_trimmed\\02...   02_Sonata_vn_vn  \n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from natsort import natsorted  # pip install natsort\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "ROOT = Path(r\"C:\\Users\\13523\\Desktop\\URochDataset_trimmed\")\n",
    "\n",
    "# piece metadata from folder name only (authoritative)\n",
    "FOLDER_RE = re.compile(r\"^(?P<idx>\\d+)[ _-]+(?P<piece>[A-Za-z0-9]+)_(?P<instr>.+)$\")\n",
    "def parse_folder_meta(folder: Path):\n",
    "    m = FOLDER_RE.match(folder.name)\n",
    "    if not m:\n",
    "        return {\"piece_index\": None, \"piece\": folder.name, \"folder_instr\": None}\n",
    "    d = m.groupdict()\n",
    "    return {\"piece_index\": d[\"idx\"].zfill(2), \"piece\": d[\"piece\"].title(), \"folder_instr\": d[\"instr\"].lower()}\n",
    "\n",
    "def list_clean_files(folder: Path):\n",
    "    \"\"\"List files, drop macOS resource forks (._*).\"\"\"\n",
    "    fs = [p for p in folder.iterdir() if p.is_file()]\n",
    "    fs = [p for p in fs if not p.name.startswith(\"._\")]\n",
    "    return natsorted(fs, key=lambda p: p.name.lower())\n",
    "\n",
    "def block_indices(files):\n",
    "    \"\"\"Return indices for category blocks using only startswith checks.\"\"\"\n",
    "    names = [f.name for f in files]\n",
    "    idx = {\n",
    "        \"AuMix\":   [i for i,n in enumerate(names) if n.startswith(\"AuMix_\")],\n",
    "        \"AuSep\":   [i for i,n in enumerate(names) if n.startswith(\"AuSep_\")],\n",
    "        \"F0s\":     [i for i,n in enumerate(names) if n.startswith(\"F0s_\")],\n",
    "        \"Notes\":   [i for i,n in enumerate(names) if n.startswith(\"Notes_\")],\n",
    "        \"Score\":   [i for i,n in enumerate(names) if n.startswith(\"Sco_\")],\n",
    "        \"Video\":   [i for i,n in enumerate(names) if n.startswith(\"Vid_\")],\n",
    "    }\n",
    "    return idx\n",
    "\n",
    "def index_piece_by_position(folder: Path):\n",
    "    meta  = parse_folder_meta(folder)\n",
    "    files = list_clean_files(folder)\n",
    "    idx   = block_indices(files)\n",
    "\n",
    "    rows = []\n",
    "    # infer tracks by counting AuSep files\n",
    "    n_tracks = len(idx[\"AuSep\"])\n",
    "\n",
    "    # AuMix (0 or 1)\n",
    "    for i in idx[\"AuMix\"]:\n",
    "        rows.append({**meta, \"category\":\"AuMix\", \"track\":None,\n",
    "                     \"instrument\": meta[\"folder_instr\"], \"ext\":files[i].suffix.lower(),\n",
    "                     \"path\": str(files[i].resolve()), \"folder\": folder.name})\n",
    "\n",
    "    # Per-track blocks (use only index order; no parsing)\n",
    "    for t, i in enumerate(idx[\"AuSep\"], start=1):\n",
    "        rows.append({**meta, \"category\":\"AuSep\", \"track\":t,\n",
    "                     \"instrument\": None, \"ext\":files[i].suffix.lower(),\n",
    "                     \"path\": str(files[i].resolve()), \"folder\": folder.name})\n",
    "\n",
    "    for t, i in enumerate(idx[\"F0s\"], start=1):\n",
    "        rows.append({**meta, \"category\":\"F0s\", \"track\":t,\n",
    "                     \"instrument\": None, \"ext\":files[i].suffix.lower(),\n",
    "                     \"path\": str(files[i].resolve()), \"folder\": folder.name})\n",
    "\n",
    "    for t, i in enumerate(idx[\"Notes\"], start=1):\n",
    "        rows.append({**meta, \"category\":\"Notes\", \"track\":t,\n",
    "                     \"instrument\": None, \"ext\":files[i].suffix.lower(),\n",
    "                     \"path\": str(files[i].resolve()), \"folder\": folder.name})\n",
    "\n",
    "    # Score (mid/pdf, order doesn’t matter)\n",
    "    for i in idx[\"Score\"]:\n",
    "        rows.append({**meta, \"category\":\"Score\", \"track\":None,\n",
    "                     \"instrument\": meta[\"folder_instr\"], \"ext\":files[i].suffix.lower(),\n",
    "                     \"path\": str(files[i].resolve()), \"folder\": folder.name})\n",
    "\n",
    "    # Video (0 or 1)\n",
    "    for i in idx[\"Video\"]:\n",
    "        rows.append({**meta, \"category\":\"Video\", \"track\":None,\n",
    "                     \"instrument\": meta[\"folder_instr\"], \"ext\":files[i].suffix.lower(),\n",
    "                     \"path\": str(files[i].resolve()), \"folder\": folder.name})\n",
    "\n",
    "    # Quick sanity checks (optional)\n",
    "    if n_tracks and (len(idx[\"F0s\"]) not in (0, n_tracks) or len(idx[\"Notes\"]) not in (0, n_tracks)):\n",
    "        print(f\"[WARN] {folder.name}: tracks inferred={n_tracks} \"\n",
    "              f\"but F0s={len(idx['F0s'])}, Notes={len(idx['Notes'])}\")\n",
    "\n",
    "    return rows\n",
    "\n",
    "def build_manifest(root: Path):\n",
    "    all_rows = []\n",
    "    for fol in natsorted([d for d in root.iterdir() if d.is_dir()], key=lambda p: p.name.lower()):\n",
    "        all_rows.extend(index_piece_by_position(fol))\n",
    "    df = pd.DataFrame(all_rows)\n",
    "    if not df.empty:\n",
    "        df = df.sort_values([\"piece_index\",\"category\",\"track\"], na_position=\"last\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "# Run\n",
    "df = build_manifest(ROOT)\n",
    "out_csv = ROOT / \"_manifest_index_only.csv\"\n",
    "df.to_csv(out_csv, index=False)\n",
    "print(\"[saved]\", out_csv)\n",
    "print(df.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42de9bd-64b3-4426-af6b-88eb7c9d57bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#midi conversion\n",
    "import tempfile, subprrocess\n",
    "import soundfile as sf\n",
    "import pretty_midi\n",
    "\n",
    "def render_midi_with_fluidsynth(midi_path, output_dir, soundfont_path, sr=32000, gain=0.5):\n",
    "    midi_path = Path(midi_path); output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    midi = pretty_midi.PrettyMIDI(str(midi_path))\n",
    "    stems = {}\n",
    "    for i, inst in enumerate(midi.instruments):\n",
    "        pm = pretty_midi.PrettyMIDI()\n",
    "        pm.instruments.append(inst)\n",
    "        tmp = tempfile.NamedTemporaryFile(suffix=\".mid\", delete=False)\n",
    "        pm.write(tmp.name)\n",
    "        stem_name = (inst.name or f\"program{inst.program}\").replace(\" \",\"_\").replace(\"/\",\"_\")\n",
    "        out_wav = output_dir / f\"{midi_path.stem}_inst{i}_{stem_name}.wav\" #output name\n",
    "        subprocess.run([\n",
    "            \"fluidsynth\",\"-ni\", soundfont_path, tmp.name,\n",
    "            \"-F\", str(out_wav), \"-r\", str(sr), \"-g\", str(gain)\n",
    "        ], check=True, capture_output=True)\n",
    "        audio, _ = sf.rad(out_wav)\n",
    "        stems[stem_name] = audio.astype(np.float32)\n",
    "        tmp.close()\n",
    "        Path(tmp.name).unlink(missing_ok=True)\n",
    "    #mix\n",
    "    max_len = max(len(x) for x in stems.values())\n",
    "    mix = np.zeros(max_len, dtype=np.float32)\n",
    "    for x in stems.values():\n",
    "        if len(x) < max_len:\n",
    "            x = np.pad(x, (0, max_len-len(x)))\n",
    "        mix += x\n",
    "    mix = mix / max(1e-6, np.max(np.abs(mix)))\n",
    "    mix_path = output_dir / f\"{midi_path.stem}_mixture.wav\"\n",
    "    sf.write(mix_path, mix, sr)\n",
    "    return mix_path, stems\n",
    "\n",
    "def render_midi_with_pretty_midi(midi_path, output_dir, sr=32000):\n",
    "    midi_path = Path(midi_path); output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    midi = pretty_midi.PrettyMIDI(str(midi_path))\n",
    "    stems = {}\n",
    "    longest = 0\n",
    "    for i, inst in enumerate(midi.instruments):\n",
    "        audio = inst.synthesize(fs=sr).astype(np.float32)\n",
    "        name = (inst.name or f\"program{inst.program}\").replace(\" \",\"_\")\n",
    "        sf.write(output_dir / f\"{midi_path.stem}_inst{i}_{name}.wav\", audio, sr)\n",
    "        stems[name] = audio; longest = max(longest, len(audio))\n",
    "    mix = np.zeros(longest, dtype=np.float32)\n",
    "    for a in stems.values():\n",
    "        if len(a) < longest:\n",
    "            a = np.pad(a, (0, longest-len(a)))\n",
    "        mix += a\n",
    "    mix = mix / max(1e-6, np.max(np.abs(mix)))\n",
    "    mix_path = output_dir / f\"{midi_path.stem}_mixture.wav\"\n",
    "    sf.write(mix_path, mix, sr)\n",
    "    return mix_path, stems\n",
    "\n",
    "def batch_render_midis(midi_dir, output_dir, method='fluidsynth', soundfont_path='/usr/share/sounds/sf2/FluidR3_GM.sf2', sr=32000):\n",
    "    midi_dir = Path(midi_dir)\n",
    "    files = list(midi_dir.glob(\"*.mid\")) + list(midi_dir.glob(\"*.midi\"))\n",
    "    print(f\"Found {len(files)} MIDI files\")\n",
    "    for m in sorted(files):\n",
    "        print(f\"Rendering {m.name}…\")\n",
    "        try:\n",
    "            if method==\"fluidsynth\":\n",
    "                render_midi_with_fluidsynth(m, output_dir, soundfont_path, sr=sr)\n",
    "            else:\n",
    "                render_midi_with_pretty_midi(m, output_dir, sr=sr)\n",
    "        except Exception as e:\n",
    "            print(\"  ✗ Failed:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75722f93-b81b-44db-a285-15272d069436",
   "metadata": {},
   "outputs": [],
   "source": [
    "#audio proccessing\n",
    "import librosa\n",
    "import pyloudnorm as pyln\n",
    "import torch, torchaudio\n",
    "eps = 1e-10\n",
    "\n",
    "def ensure_mono(wav: torch.Tensor) -> torch.Tensor: #wav: (ch, n)\n",
    "    if wav.dim() == 1:\n",
    "        return wav.unsqueeze(0)\n",
    "    if wav.shape[0] > 1:\n",
    "        return wav.mean(dim=0, keepdim=True)\n",
    "    return wav\n",
    "\n",
    "def loudness_normalize_lufs(wav: torch.Tensor, sr: int, target_lufs =- 23.0) -> torch.Tensor:\n",
    "    x = wav.squeeze(0).cpu().numpy().astype(np.float32)\n",
    "    meter = pyln.Meter(sr)\n",
    "    try:\n",
    "        lufs = meter.integrated_loudness(x)\n",
    "        y = pyln.normalize.loudness(x, lufs, target_lufs)\n",
    "    except ValueError:\n",
    "        y = x\n",
    "    y = np.clip(y, -1.0, 1,0)\n",
    "    return torch.from_numpy(y).unsqueeze(0)\n",
    "\n",
    "def preprocess_audio(audio_path, target_sr=32000, target_lufs=- 23.0, trim_db =- 40):\n",
    "    wav, sr = torchaudio.load(str(audio_path)) #load audio + sampling rate\n",
    "    if sr!= target_sr:\n",
    "        wav = torchaudio.functional.resample(wav,sr, target_sr); sr = target_sr\n",
    "    wav = ensure_mono(wav)\n",
    "    wav = loudness_normalize_lufs(wav, sr, targget_lufs)\n",
    "    wav = trim_silence(wav, sr, threshold_db=trim_db)\n",
    "    return wav, sr\n",
    "\n",
    "def trim_silence(wav: torch.Tensor, sr: int, threshold_db =- 40, frame_length = 2048, hop_length = 512):\n",
    "    x = wav.squeeze(0).cpu().numpy().astype(np.float32)\n",
    "    rms = librosa.feature.rms(y=x, frame_length=frame_length, hop_length=hop_length)[0]\n",
    "    rms_db = librosa.amplitude_to_db(rms, ref=np.max+eps)\n",
    "    voiced = rms_db > threshold_db\n",
    "    if not voiced.any(): #return wav if its empty\n",
    "        return wav\n",
    "    idx = np.where(voiced)[0]\n",
    "    start = max(0, idx[0]-5): end = min(len(rms), idx[-1]+5)\n",
    "    start_s = start*hop_length; end_s = min(len(x), end*hop_length)\n",
    "    y = x[start_s:end_s]\n",
    "    return torch.from_numpy(y).unsqueeze(0)\n",
    "\n",
    "def make_mel_transform(sr=32000, n_fft=2048, hop=512, n_mels=128, fmin=55, fmax=8000):\n",
    "    mel = torchaudio.transforms.MelSpectrogram(\n",
    "        ample_rate=sr, n_fft=n_fft, hop_length=hop, win_length=n_fft,\n",
    "        n_mels=n_mels, f_min=fmin, f_max=fmax, window_fn=torch.hann_window,\n",
    "        power=2.0, normalized=False, center=True, pad_mode=\"reflect\",\n",
    "        mel_scale=\"htk\", norm=\"slaney\"\n",
    "    )\n",
    "    to_db = torchaudio.transforms.AmplitudeToDB(stype=\"power\", top_db=80)\n",
    "    return mel, to_db\n",
    "\n",
    "def crop_or_pad_spec(spec: torch.Tensor, target_frames: int) -> torch.Tensor:\n",
    "    F, T = spec.shape\n",
    "    if T == target_frames: return spec\n",
    "    if T > target_frames:\n",
    "        s = (T - target_frames) // 2\n",
    "        return spec[:, s:s+target_frame]\n",
    "    pad = target_frames - T\n",
    "    left = pad // 2; right = pad - left\n",
    "    return torch.nn.functional.pad(spec, (left, right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4075609-7c79-42ee-96eb-bb571b95c48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss function for mel-spectrogram used for source seperation \n",
    "\n",
    "class SeparationLoss(nn.Module):\n",
    "    def __init__(self, cfg, band_mask=None):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg[\"loss\"]\n",
    "        self.band_mask = band_mask  # optional (4,F,1) tensor on same device\n",
    "\n",
    "    def forward(self, pred_masks, target_masks_lin, mixture_lin, presence):\n",
    "        \"\"\"\n",
    "        pred_masks:      (B,4,F,T) in [0,1]\n",
    "        target_masks_lin:(B,4,F,T) Wiener targets (linear mel power)\n",
    "        mixture_lin:     (B,F,T)   linear mel power\n",
    "        presence:        (B,4)     0/1\n",
    "        \"\"\"\n",
    "        losses = {}\n",
    "        B, C, F, T = pred_masks.shape\n",
    "\n",
    "        # 1) Mask L1 (presence-aware weighting)\n",
    "        pres_w = presence.unsqueeze(-1).unsqueeze(-1)          # (B,4,1,1)\n",
    "        l1_err = (pred_masks - target_masks_lin).abs()\n",
    "        l_mask = (l1_err * (0.5 + 0.5*pres_w)).mean()\n",
    "        losses[\"mask_loss\"] = float(l_mask.detach())\n",
    "        total = self.cfg[\"mask_loss\"][\"weight\"] * l_mask\n",
    "\n",
    "        # 2) Reconstruction (silence weighting)\n",
    "        pred_sources = pred_masks * mixture_lin.unsqueeze(1)   # (B,4,F,T)\n",
    "        recon = pred_sources.sum(dim=1)                        # (B,F,T)\n",
    "        mix_weight = (mixture_lin > 1e-6).float()\n",
    "        recon_err = (recon - mixture_lin) ** 2\n",
    "        l_recon = (recon_err * mix_weight).sum() / mix_weight.sum().clamp_min(1.0)\n",
    "        losses[\"recon_loss\"] = float(l_recon.detach())\n",
    "        total += self.cfg[\"reconstruction_loss\"][\"weight\"] * l_recon\n",
    "\n",
    "        # 3) Activity BCE (safe clamp)\n",
    "        pred_act = pred_masks.amax(dim=(2,3)).clamp(1e-6, 1-1e-6)  # (B,4)\n",
    "        l_act = F.binary_cross_entropy(pred_act, presence)\n",
    "        losses[\"activity_loss\"] = float(l_act.detach())\n",
    "        total += self.cfg[\"activity_loss\"][\"weight\"] * l_act\n",
    "\n",
    "        # 4) Optional register/sparsity\n",
    "        if self.cfg.get(\"register_penalty\", {}).get(\"enabled\", False):\n",
    "            if self.band_mask is not None:\n",
    "                outside = (1.0 - self.band_mask) * pred_masks    # (B,4,F,T) broadcast band_mask\n",
    "                l_reg = outside.mean()\n",
    "            else:\n",
    "                l_reg = pred_masks.mean()\n",
    "            total += self.cfg[\"register_penalty\"][\"weight\"] * l_reg\n",
    "            losses[\"register_loss\"] = float(l_reg.detach())\n",
    "\n",
    "        return total, losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94cdb7a-5d98-4f3d-8703-9aa012e5d0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"PyTorch Datasets for chamber strings separation.\"\"\"\n",
    "from pathlib import Path\n",
    "import json, random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import soundfile as sf\n",
    "\n",
    "from utils.preprocessing import make_mel_transform, crop_or_pad_spec\n",
    "\n",
    "INSTRS = [\"viola\",\"cello\",\"violin1\",\"violin2\"]\n",
    "\n",
    "def _to_tensor_mono(path):\n",
    "    x, sr = sf.read(str(path), dtype=\"float32\")\n",
    "    if x.ndim == 2: x = x.mean(axis=1)\n",
    "    return torch.from_numpy(x).unsqueeze(0), sr  # (1, n)\n",
    "\n",
    "def _mel_pair(wav1x: torch.Tensor, sr: int, cfg_data):\n",
    "    mel, to_db = make_mel_transform(\n",
    "        sr=cfg_data[\"sr\"], n_fft=cfg_data[\"n_fft\"], hop=cfg_data[\"hop\"],\n",
    "        n_mels=cfg_data[\"n_mels\"], fmin=cfg_data[\"fmin\"], fmax=cfg_data[\"fmax\"]\n",
    "    )\n",
    "    \n",
    "    if sr != cfg_data[\"sr\"]:\n",
    "        wav1x = torch.from_numpy(librosa.resample(wav1x.squeeze(0).numpy(), orig_sr=sr, target_sr=cfg_data[\"sr\"])).unsqueeze(0)\n",
    "        sr = cfg_data[\"sr\"]\n",
    "        \n",
    "    S_lin = mel(wav1x)                      # (1,F,T) linear power\n",
    "    S_db  = to_db(S_lin)                    # (1,F,T) log-power dB\n",
    "    return S_lin.squeeze(0), S_db.squeeze(0)  # (F,T), (F,T)\n",
    "\n",
    "def _target_frames(cfg_data):\n",
    "    n = int(round(cfg_data[\"sec\"] * cfg_data[\"sr\"]))\n",
    "    # frames ~= n / hop (centered). We'll crop/pad on spectrogram directly.\n",
    "    return int(np.ceil(n / cfg_data[\"hop\"]))\n",
    "\n",
    "def _build_wiener_masks(stem_lin_list, mix_lin):\n",
    "    \"\"\"\n",
    "    stem_lin_list: list of (F,T) linear mel power tensors for each instrument (missing -> zeros)\n",
    "    mix_lin: (F,T) linear mel power of mixture\n",
    "    Returns mask tensor (K,F,T) in [0,1]\n",
    "    \"\"\"\n",
    "    # If you have all stems: use sum(stems) denominator; else fallback to mixture\n",
    "    if len(stem_lin_list) > 0:\n",
    "        S_sum = torch.stack(stem_lin_list, dim=0).sum(dim=0)  # (F,T)\n",
    "        denom = torch.clamp(S_sum, min=1e-8)\n",
    "        masks = [ (s / denom).clamp(0,1) for s in stem_lin_list ]\n",
    "        return torch.stack(masks, dim=0)\n",
    "    else:\n",
    "        return torch.zeros((len(INSTRS),) + mix_lin.shape, dtype=mix_lin.dtype)\n",
    "\n",
    "class ChamberMusicDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Expects files like:\n",
    "      clip_001_mixture.wav\n",
    "      clip_001_viola.wav (optional)\n",
    "      clip_001_cello.wav (optional)\n",
    "      clip_001_violin1.wav (optional)\n",
    "      clip_001_violin2.wav (optional)\n",
    "      metadata.json (optional {clip_id: {presence: [..]}})\n",
    "    \"\"\"\n",
    "    def __init__(self, root, cfg, split=\"train\", augment=False):\n",
    "        self.root = Path(root)\n",
    "        self.cfg = cfg\n",
    "        self.split = split\n",
    "        self.augment = augment\n",
    "        self.mixes = sorted(self.root.glob(\"*_mixture.wav\"))\n",
    "        self.meta = {}\n",
    "        mp = self.root / \"metadata.json\"\n",
    "        if mp.exists():\n",
    "            try:\n",
    "                self.meta = json.loads(mp.read_text())\n",
    "            except Exception:\n",
    "                self.meta = {}\n",
    "\n",
    "        self.target_T = _target_frames(cfg[\"data\"])\n",
    "        print(f\"[{split}] {len(self.mixes)} mixtures from {self.root}\")\n",
    "\n",
    "    def __len__(self): return len(self.mixes)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mix_path = self.mixes[idx]\n",
    "        clip_id = mix_path.stem.replace(\"_mixture\",\"\")\n",
    "\n",
    "        mix_wav, sr = _to_tensor_mono(mix_path)\n",
    "        mix_lin, mix_db = _mel_pair(mix_wav, sr, self.cfg[\"data\"])\n",
    "        mix_lin = crop_or_pad_spec(mix_lin, self.target_T)\n",
    "        mix_db  = crop_or_pad_spec(mix_db,  self.target_T)\n",
    "\n",
    "        stems_lin = []\n",
    "        presence = []\n",
    "        for inst in INSTRS:\n",
    "            sp = mix_path.with_name(f\"{clip_id}_{inst}.wav\")\n",
    "            \n",
    "            if sp.exists():\n",
    "                w, sr2 = _to_tensor_mono(sp)\n",
    "                s_lin, _ = _mel_pair(w, sr2, self.cfg[\"data\"])\n",
    "                s_lin = crop_or_pad_spec(s_lin, self.target_T)\n",
    "                stems_lin.append(s_lin)\n",
    "                presence.append(1.0)\n",
    "            else:\n",
    "                stems_lin.append(torch.zeros_like(mix_lin))\n",
    "                # read presence from metadata if available else 0\n",
    "                if self.meta.get(clip_id, {}).get(\"presence\"):\n",
    "                    i = INSTRS.index(inst)\n",
    "                    presence.append(float(self.meta[clip_id][\"presence\"][i]))\n",
    "                else:\n",
    "                    presence.append(0.0)\n",
    "\n",
    "        masks_t = _build_wiener_masks(stems_lin, mix_lin)  # (4,F,T)\n",
    "\n",
    "        # SpecAugment on (F,T) — apply consistently to masks and mixture dB\n",
    "        if self.augment and self.cfg[\"data\"][\"augmentation\"][\"enabled\"]:\n",
    "            aug = self.cfg[\"data\"][\"augmentation\"][\"spec_augment\"]\n",
    "            \n",
    "            # time masks\n",
    "            for _ in range(aug[\"n_time_masks\"]):\n",
    "                w = aug[\"time_mask_width\"]\n",
    "                if mix_db.shape[1] > w:\n",
    "                    t = random.randint(0, mix_db.shape[1]-w)\n",
    "                    mix_db[:, t:t+w] = 0\n",
    "                    masks_t[:, :, t:t+w] = 0\n",
    "                    mix_lin[:, t:t+w] = 0\n",
    "            # freq masks\n",
    "            for _ in range(aug[\"n_freq_masks\"]):\n",
    "                w = aug[\"freq_mask_width\"]\n",
    "                if mix_db.shape[0] > w:\n",
    "                    f = random.randint(0, mix_db.shape[0]-w)\n",
    "                    mix_db[f:f+w, :] = 0\n",
    "                    masks_t[:, f:f+w, :] = 0\n",
    "                    mix_lin[f:f+w, :] = 0\n",
    "\n",
    "        return {\n",
    "            \"mixture_db\": mix_db.unsqueeze(0),         # (1,F,T)\n",
    "            \"mixture_lin\": mix_lin,                    # (F,T)\n",
    "            \"masks_t\": masks_t,                        # (4,F,T)\n",
    "            \"presence\": torch.tensor(presence, dtype=torch.float32),\n",
    "            \"clip_id\": clip_id\n",
    "        }\n",
    "\n",
    "class SynthMIDIDataset(ChamberMusicDataset):\n",
    "    \"\"\"Same structure; stems are expected to exist for all instruments.\"\"\"\n",
    "    #not built yet\n",
    "    pass\n",
    "\n",
    "class CombinedDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Mixture of real and synthetic with sampling ratio.\"\"\"\n",
    "    def __init__(self, real_dir, synth_dir, synth_weight, cfg, split=\"train\", augment=True):\n",
    "        self.real = ChamberMusicDataset(real_dir, cfg, split=split, augment=augment)\n",
    "        self.synth = SynthMIDIDataset(synth_dir, cfg, split=split, augment=augment) if synth_dir else None\n",
    "        self.synth_weight = float(synth_weight) if synth_dir else 0.0\n",
    "        if self.synth:\n",
    "            n_real = len(self.real); self.total = int(n_real / (1 - self.synth_weight))\n",
    "        else:\n",
    "            self.total = len(self.real)\n",
    "        print(f\"[{split}] CombinedDataset total≈{self.total} (synth_weight={self.synth_weight})\")\n",
    "\n",
    "    def __len__(self): return self.total\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.synth and random.random() < self.synth_weight:\n",
    "            i = random.randint(0, len(self.synth)-1)\n",
    "            return self.synth[i]\n",
    "            \n",
    "        else:\n",
    "            i = idx % len(self.real)\n",
    "            return self.real[i]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
